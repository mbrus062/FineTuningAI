#!/usr/bin/env python3
import argparse
import csv
import os
import re

from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set, Tuple

_STOPWORDS = {"of", "on", "the", "a", "an", "and"}

def _norm_for_match(s: str) -> str:
    """
    Lowercase, replace non-alnum with spaces, collapse whitespace.
    Keeps it substring-friendly.
    """
    s = s.lower()
    s = re.sub(r"[^a-z0-9]+", " ", s)
    return " ".join(s.split())

def _strip_stopwords(norm_s: str) -> str:
    toks = [t for t in norm_s.split() if t and t not in _STOPWORDS]
    return " ".join(toks)

def _token_set(norm_s: str) -> Set[str]:
    return set(t for t in norm_s.split() if t and t not in _STOPWORDS)

def _title_similarity_score(desired_norm: str, candidate_norm: str) -> int:
    """
    Simple, fast relevance score for author-only hits:
    token overlap count (stopwords removed).
    """
    dt = _token_set(desired_norm)
    ct = _token_set(candidate_norm)
    return len(dt & ct)

ROMAN_RE = re.compile(r"^(?=[ivxlcdm]+$)[ivxlcdm]+$", re.IGNORECASE)

_ROMAN_MAP = {
    "I": 1, "V": 5, "X": 10, "L": 50,
    "C": 100, "D": 500, "M": 1000,
}

def roman_to_int(s: str) -> int:
    """
    Convert a Roman numeral (I, II, IV, IX, XII, etc.) to an int.
    Returns a large number if it doesn't look like a roman numeral.
    """
    if not s:
        return 10**9
    s = s.strip().upper()
    if not ROMAN_RE.match(s):
        return 10**9

    total = 0
    prev = 0
    for ch in reversed(s):
        val = _ROMAN_MAP.get(ch, 0)
        if val < prev:
            total -= val
        else:
            total += val
            prev = val
    return total

from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

DEFAULT_ROOT = "/ai_data/ebooks"

INV_DIR = Path.home() / "FineTuningAI" / "inventory"
CHECKLIST_DIR = INV_DIR / "checklists"

DEFAULT_OUT = str(INV_DIR / "ebooks_inventory.csv")
DEFAULT_WISHLIST_PATH = str(INV_DIR / "ebooks_wishlist.txt")
DEFAULT_CHECKLISTS = sorted([str(p) for p in CHECKLIST_DIR.glob("*.txt")])

EXTS = {".pdf", ".epub", ".mobi", ".djvu", ".txt", ".docx", ".odt", ".htm", ".html", ".rtf", ".azw", ".azw3", ".azw4"}

# common “noise” dirs; you can add/remove anytime
DEFAULT_EXCLUDE_DIRS = {"_imports", ".git", ".venv", "__pycache__", ".cache"}

def normalize(s: str) -> str:
    s = s.lower()
    s = s.replace("&", "and")
    s = re.sub(r"[^a-z0-9]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def safe_read_text(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        return path.read_text(encoding="latin-1", errors="ignore")

def iter_book_files(root: Path, exts: set, exclude_dirs: set) -> Iterable[Path]:
    # skip excluded subtrees early
    for p in root.rglob("*"):
        if p.is_dir():
            continue
        if p.suffix.lower() not in exts:
            continue
        # exclude if any directory component is in exclude_dirs
        parts = set(p.parts)
        if parts & exclude_dirs:
            continue
        yield p

@dataclass
class BookFile:
    path: Path
    name: str
    ext: str
    size: int
    mtime_iso: str
    norm: str

def build_index(root: Path, exts: set, exclude_dirs: set) -> List[BookFile]:
    out: List[BookFile] = []
    for p in iter_book_files(root, exts, exclude_dirs):
        try:
            st = p.stat()
        except FileNotFoundError:
            continue
        out.append(
            BookFile(
                path=p,
                name=p.name,
                ext=p.suffix.lower(),
                size=int(st.st_size),
                mtime_iso=datetime.fromtimestamp(st.st_mtime).isoformat(timespec="seconds"),
                norm=normalize(str(p)),
            )
        )
    # stable, useful ordering
    out.sort(key=lambda bf: (normalize(bf.name), str(bf.path)))
    return out

def write_inventory_csv(files: List[BookFile], out_csv: Path) -> None:
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    with out_csv.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["filename", "ext", "bytes", "modified", "path"])
        for bf in files:
            w.writerow([bf.name, bf.ext, bf.size, bf.mtime_iso, str(bf.path)])

def parse_checklist_line(line: str) -> Optional[Tuple[str, str]]:
    # Accept:
    # - "Author | Title"
    # - "Author | Title;AltTitle;AltTitle2"
    # ignore status annotations if present
    line = line.strip()
    if not line or line.startswith("#"):
        return None
    if line.startswith("-"):
        line = line[1:].strip()

    if "|" not in line:
        return None
    author, title = [x.strip() for x in line.split("|", 1)]
    if not author or not title:
        return None
    return author, title

def match_checklist(author: str, title_field: str, files: List[BookFile]) -> Tuple[str, List[Path]]:
    # title_field may contain synonyms separated by ';'
    author_n = normalize(author)
    title_variants = [normalize(x.strip()) for x in title_field.split(";") if x.strip()]
    if not title_variants:
        title_variants = [normalize(title_field)]

    primary_hits: List[Path] = []
    author_only_hits: List[Path] = []

    # Pre-normalize variants once
    title_variants_norm = [_norm_for_match(tv) for tv in title_variants if tv]
    title_variants_sw   = [_strip_stopwords(tv) for tv in title_variants_norm if tv]

    # A “best representative” title for scoring (first variant is fine)
    desired_title_norm = title_variants_norm[0] if title_variants_norm else ""

    for bf in files:
        bf_norm = bf.norm  # already normalized path text
        bf_norm2 = _norm_for_match(bf_norm)
        bf_norm2_sw = _strip_stopwords(bf_norm2)

        has_author = (author_n in bf_norm) if author_n else False

        has_title = any(
            (tv in bf_norm2) or (tv_sw and tv_sw in bf_norm2_sw)
            for tv, tv_sw in zip(title_variants_norm, title_variants_sw)
        )

        if has_author and has_title:
            primary_hits.append(bf.path)
        elif has_author:
            author_only_hits.append(bf.path)

    if primary_hits:
        return "PRESENT_PRIMARY", sorted(primary_hits, key=lambda p: author_roman_sort_key(author, str(p)))

    if author_only_hits:
        # Rank author-only hits by similarity to desired title, then fall back to roman sort
        scored = []
        for p in author_only_hits:
            cand_norm = _norm_for_match(str(p))
            score = _title_similarity_score(desired_title_norm, cand_norm)
            scored.append((score, p))

        scored.sort(key=lambda sp: (-sp[0], author_roman_sort_key(author, str(sp[1]))))
        return "PRESENT_AUTHOR_ONLY", [p for _, p in scored]

    return "MISSING", []





def write_checklist_report(checklists: List[str], files: List[BookFile], root: Path, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    now = datetime.now().isoformat(timespec="seconds")
    total = len(files)

    lines: List[str] = []
    lines.append(f"Checklist report generated: {now}")
    lines.append(f"Scanned root: {root}")
    lines.append(f"Total indexed files: {total}")
    lines.append("")

    for cl in checklists:
        clp = Path(cl)
        lines.append(f"=== CHECKLIST: {clp} ===")

        entries: List[Tuple[str, str]] = []
        if clp.exists():
            for raw in safe_read_text(clp).splitlines():
                parsed = parse_checklist_line(raw)
                if parsed:
                    entries.append(parsed)
        # Alphabetize by (author, title) in the report (your request)
        entries.sort(key=lambda x: (normalize(x[0]), normalize(x[1])))

        primary = author_only = missing = 0
        for author, title_field in entries:
            status, hits = match_checklist(author, title_field, files)
            if status == "PRESENT_PRIMARY":
                primary += 1
            elif status == "PRESENT_AUTHOR_ONLY":
                author_only += 1
            else:
                missing += 1

            lines.append(f"- {author} | {title_field} : {status}")
            for hp in hits:
                lines.append(f"    {hp}")
        lines.append("")
        lines.append(f"Summary: PRIMARY={primary}  AUTHOR_ONLY={author_only}  MISSING={missing}")
        lines.append("")

    out_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")

def parse_wishlist_lines(path: Path) -> List[Tuple[str, str]]:
    # Accept:
    # 1) "Label (any) (query words)"
    # 2) "Label | query words"
    items: List[Tuple[str, str]] = []
    if not path.exists():
        return items
    for raw in safe_read_text(path).splitlines():
        s = raw.strip()
        if not s or s.startswith("#"):
            continue
        if "|" in s:
            label, q = [x.strip() for x in s.split("|", 1)]
            if label and q:
                items.append((label, q))
            continue
        # last (...) is treated as query
        m = re.findall(r"\(([^()]*)\)", s)
        if m:
            q = m[-1].strip()
            label = re.sub(r"\([^()]*\)\s*$", "", s).strip()
            if label and q:
                items.append((label, q))
    # Alphabetize by label
    items.sort(key=lambda x: normalize(x[0]))
    return items

def author_roman_sort_key(author: str, path: str) -> tuple[int, str]:
    """
    Generic sorter for Loeb-style volumes:
      "... Cicero XXII - ..."
      "... Aristotle IV - ..."
      "... Augustine X - ..."
    Non-matching items sort last.
    """
    if not author:
        return (10**9, path)

    # Use first word of author as token (e.g. "Seneca the Younger" → "Seneca")
    token = author.strip().split()[0]

    m = re.search(
        rf"\b{re.escape(token)}\b\s+([ivxlcdm]+)\b",
        path,
        flags=re.IGNORECASE,
    )

    if not m:
        return (10**9, path)

    return (roman_to_int(m.group(1)), path)


def wishlist_hits(query: str, files: List[BookFile]) -> List[str]:
    """
    query:
      - plain substring match against normalized file paths (BookFile.norm)
      - OR regex if it starts with 're:'
    returns: list of matching *paths as strings*
    """
    q = query.strip()

    # Regex mode: query starts with "re:"
    if q.lower().startswith("re:"):
        pattern = q[3:].strip()
        try:
            rx = re.compile(pattern, flags=re.IGNORECASE)
        except re.error:
            return []

        hits = [str(bf.path) for bf in files if rx.search(bf.norm)]

        # Optional: if the regex is specifically targeting Philo + roman numerals,
        # we can sort those numerically (we'll refine this next).
        hits.sort(key=lambda s: author_roman_sort_key("Philo", s))

        return hits

    # Substring mode
    qn = normalize(q)
    hits = [str(bf.path) for bf in files if qn in bf.norm]
    return hits





def write_wishlist_report(wishlist_path: Path, files: List[BookFile], root: Path, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    now = datetime.now().isoformat(timespec="seconds")
    total = len(files)

    lines: List[str] = []
    lines.append(f"Wishlist report generated: {now}")
    lines.append(f"Scanned root: {root}")
    lines.append(f"Total indexed files: {total}")
    lines.append(f"Wishlist file: {wishlist_path}")
    lines.append("")

    items = parse_wishlist_lines(wishlist_path)
    for label, query in items:
        hits = wishlist_hits(query, files)
        status = "PRESENT" if hits else "MISSING"
        lines.append(f"- {query} ({label}): {status} ({len(hits)} hit(s))")
        if hits:
            for hp in hits:
                lines.append(f"    {hp}")
        lines.append("")


    out_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")

def main() -> int:
    ap = argparse.ArgumentParser(description="Ebook inventory + checklist + wishlist reports")
    ap.add_argument("--root", default=DEFAULT_ROOT, help=f"Root to scan (default: {DEFAULT_ROOT})")
    ap.add_argument("--out", default=DEFAULT_OUT, help="Inventory CSV output path")
    ap.add_argument("--wishlist", default=DEFAULT_WISHLIST_PATH, help="Wishlist file path")
    ap.add_argument("--checklists", nargs="*", default=DEFAULT_CHECKLISTS, help="Checklist file(s) to run")
    ap.add_argument("--include-imports", action="store_true", help="Include _imports/ in scan (default excludes it)")
    ap.add_argument("--exclude-dirs", nargs="*", default=sorted(DEFAULT_EXCLUDE_DIRS), help="Dir names to exclude from scan")
    args = ap.parse_args()

    root = Path(args.root).expanduser()
    out_csv = Path(args.out).expanduser()
    wishlist_path = Path(args.wishlist).expanduser()
    checklist_report = INV_DIR / "ebooks_checklist_report.txt"
    wishlist_report = INV_DIR / "ebooks_wishlist_report.txt"

    exclude = set(args.exclude_dirs)
    if args.include_imports:
        exclude.discard("_imports")

    files = build_index(root, EXTS, exclude)

    write_inventory_csv(files, out_csv)
    write_checklist_report(args.checklists, files, root, checklist_report)
    write_wishlist_report(wishlist_path, files, root, wishlist_report)

    print("\n=== INVENTORY SUMMARY ===")
    print(f"Inventory CSV: {out_csv}")
    print(f"Wishlist report: {wishlist_report}")
    print(f"Checklist report: {checklist_report}")

    # Print full reports to terminal (your request: no truncation)
    print(f"\n--- {checklist_report} (FULL) ---")
    print(checklist_report.read_text(encoding="utf-8"))

    print(f"\n--- {wishlist_report} (FULL) ---")
    print(wishlist_report.read_text(encoding="utf-8"))

    return 0

if __name__ == "__main__":
    raise SystemExit(main())
